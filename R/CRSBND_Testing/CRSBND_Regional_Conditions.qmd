---
title: "US and DFO Regional Conditions"
format: 
  html:
    self-contained: true
---


```{r}
# Load packages
library(raster)
library(here)
library(tidyverse)
library(gmRi)
library(sf)
library(patchwork)
library(scales)



# Paths to Box Assets 
mills_path <- cs_path(box_group = "mills")
crsbnd_path <- str_c(mills_path, "Projects/Single_Species_SDMS/Results")


```

# About:

I'm tired of tracking down the baseline and +1,2,3,4C horizon period temperatures from the `sdm_workflow` repository.

This doc will cover creating new ones for the US and Candadian waters, for use on cross-boundary figures:



```{r}
#| label: US and Canada Regions


# Load the shapefiles
dfo_bounds  <- read_sf(here::here("local_data/Regions_for_CRSBND/DFO.shp"))
nmfs_bounds <- read_sf(here::here("local_data/Regions_for_CRSBND/NMFS.shp"))
land_sf     <- read_sf(here::here("COCA_SDM_app_dev/dev/scratch_data", "nw_atlantic_countries_crs32619.geojson"))
hague_sf    <- read_sf(here::here("COCA_SDM_app_dev/dev/scratch_data", "hagueline_crs32619.geojson"))


# Need to get an area per region...number of cells per region
full_reg_bounds <- bind_rows(dfo_bounds, nmfs_bounds) %>% 
  st_union() %>% 
  st_as_sf()

```

### Set SSP Data Load Options


Select Variable & SSP Scenario
 
These run one at a time, the data are stored in separate arrays.

```{r}

####  Select SSP Scenario  ####
# ssp_select <- "SSP1_26"
ssp_select <- "SSP5_85"

# Select ONE variable to use for workflow:
# var_select <- "surf_temp"
# var_select <- "surf_sal"
var_select <- "bot_temp"
# var_select <- "bot_sal"



# variables to pull
vars_neat <- c(
  "Surface Temperature" = "surf_temp",
  "Surface Salinity"    = "surf_sal", 
  "Bottom Temperature"  = "bot_temp",
  "Bottom Salinity"     = "bot_sal")
```


### load Bias Corrected SSP Scenario Data



```{r}
# Loading Bias Corrected Data for Specific Variables and from Each Scenario
# Bias Corrections Performed in :
# CMIP6_processing/R/CMIP_SODA_bias_corrections.R
# CMIP6_processing/R/CMIP_OISST_bias_corrections.R

load_bias_corrected <- function(cmip_var, ssp_scenario){
  
  # bias corrected data folders
  var_folders <- map(vars_neat, ~ cs_path(
    box_group = "res", 
    subfolder = str_c("CMIP6/", ssp_scenario, "/BiasCorrected/IndividualModels/", .x, "/")))
  var_folders <- setNames(var_folders, vars_neat)
  
  
  # list files:
  file_paths <- str_c(list.files(var_folders[[cmip_var]], full.names = TRUE, pattern = "\\.grd$"))
  file_names <- list.files(var_folders[[cmip_var]], full.names = FALSE, pattern = "\\.grd$")
  file_names <- str_remove_all(file_names, ".grd")
  file_paths <- setNames(file_paths, file_names)
  
  # load as rasters in list
  var_stacks <- map(file_paths, raster::stack)
  
  # Return the stack
  return(var_stacks)
}
```


```{r}


# Load the mean/5th/95th from ensembles, using a variable and a scenario
load_ensemble_percentiles <- function(cmip_var, ssp_scenario){
  
  # ensemble folders
  # bias corrected data folders
  ens_folders <- map(vars_neat, ~ cs_path(
    box_group = "res", 
    subfolder = str_c("CMIP6/", ssp_scenario, "/BiasCorrected/EnsembleData/", .x, "/")))
  ens_folders <- setNames(ens_folders, vars_neat)
  
  # list files:
  file_paths <- str_c(list.files(ens_folders[[cmip_var]], full.names = TRUE, pattern = "\\.grd$"))
  file_names <- list.files(ens_folders[[cmip_var]], full.names = FALSE, pattern = "\\.grd$")
  file_names <- str_remove_all(file_names, ".grd")
  file_paths <- setNames(file_paths, file_names)
  
  # load as rasters in list
  ens_stacks <- map(file_paths, raster::stack)
  
  # Return the stack
  return(ens_stacks)
}
```



```{r}

# load bias corrected data for individual scenario runs
observed_var <- load_bias_corrected(
  cmip_var = var_select, 
  ssp_scenario = ssp_select)

# Load ensemble percentile data
ensemble_var <- load_ensemble_percentiles(
  cmip_var = var_select, 
  ssp_scenario = ssp_select)

```


### Processing Functions

Each array will be cropped to te region, and then a single average for the full area is taken for each time step.

```{r}
# 1.
# Function to mask rasters using shape
mask_shape <- function(in_ras, in_mask){# Check extents
  
  # Check extent for to make sure they overlap
  # Rotate if you need to
  in_ext <- extent(in_ras)
  if(in_ext@xmax > 180){
    out_extent <- in_ext - c(360, 360, 0, 0)
    in_ras <- raster::setExtent(in_ras, out_extent)
  }
  
  # crop+mask
  r1 <- raster::crop(x = in_ras, y = in_mask)
  r2 <- raster::mask(x = r1, mask = in_mask)
  return(r2)}

```


```{r}
# 2.
# Process means and date formats for a monthly raster stack
stack_to_df <- function(month_stack, var_name){
  var_sym <- sym(var_name)
  raster::cellStats(month_stack, "mean", na.rm = T) %>% 
    raster::as.data.frame() %>% 
    tibble::rownames_to_column() %>% 
    setNames(c("date", var_name)) %>% 
    dplyr::mutate(
      date = str_remove(date, "X"),
      date = str_replace_all(date, "[.]", "-"),
      date = as.Date(str_c(date, "-15")),
      year = lubridate::year(date))
}
```


`mask_and_stack` is a wrapper which repeats these steps for a group of areas and returns them all in one table.

```{r}
# 3. Function to combine steps and run for all areas
# Mask and Stack together
# put them together, set which regions are going down the pipeline here
mask_and_stack <- function(masking_var, var_name){
  
  # Mask and get timeseries for each area
  
  # US
  masked_us   <- mask_shape(in_ras = masking_var, in_mask =  nmfs_bounds)
  masked_us   <- stack_to_df(month_stack = masked_us, var_name =  var_name)
  
  # DFO
  masked_dfo <- mask_shape(in_ras = masking_var, in_mask =  dfo_bounds)
  masked_dfo <- stack_to_df(month_stack = masked_dfo,var_name =  var_name)
  
  # Full US+Canada
  masked_overall   <- mask_shape(in_ras = masking_var, in_mask = full_reg_bounds)
  masked_overall   <- stack_to_df(month_stack = masked_overall, var_name = var_name)
  
  
  # Put in list, combine
  bind_rows(list(
    "US Survey Area"         = masked_us,
    "Canadian Survey Area"   = masked_dfo,
    "combined_surveys"       = masked_overall
  ), .id = "Region")
}

```


```{r}

# Runs one variable at a time:
masked_var <- map_dfr(
  .x = observed_var,
  .f = ~ mask_and_stack(masking_var = .x, var_name = var_select), 
  .id = "cmip_id")


# Run the percentiles too
masked_percentiles <- map_dfr(
  .x = ensemble_var,
  .f = ~ mask_and_stack(masking_var = .x, var_name = var_select), 
  .id = "cmip_id")

```


### Check Scenario Timeseries & Save

```{r}
# Combine and add metadata
var_combined <- bind_rows(
  list(masked_var, masked_percentiles)) %>% 
  mutate(
    data_source = case_when(
      str_detect(cmip_id, "percentile") ~ "Ensemble Data",
      str_detect(cmip_id, "mean")       ~ "Ensemble Data",
      str_detect(cmip_id, "historic")   ~ "CMIP6 Historical",
      TRUE ~ "CMIP6 SSP"),
    ensemble_statistic = case_when(
      str_detect(cmip_id, "95th")  ~ "95th Percentile",
      str_detect(cmip_id, "5th")   ~ "5th Percentile",
      str_detect(cmip_id, "mean")  ~ "Ensemble Mean",
      TRUE ~ "Individual CMIP6 Output"),
    ensemble_statistic = factor(
      ensemble_statistic, 
      levels = c("Individual CMIP6 Output","5th Percentile",
                 "Ensemble Mean", "95th Percentile"))
    )



##### Check With Plot  ####
# palette options
spaghetti_pal <- c(
  "Individual CMIP6 Output" = "gray",
  "5th Percentile" = "lightblue",
  "Ensemble Mean" = as.character(gmri_cols("gmri blue")),
  "95th Percentile" = "dark red")

spaghetti_sizes <- c(
  "Individual CMIP6 Output" = 0.3,
  "5th Percentile" = .75,
  "Ensemble Mean" = .75,
  "95th Percentile" = .75)

spaghetti_alpha <- c(
  "Individual CMIP6 Output" = 0.2,
  "5th Percentile" = .6,
  "Ensemble Mean" = .6,
  "95th Percentile" = .6)

# format variable for display on axis
var_titles <- c(
  "bot_temp" = expression("Bottom Temperature"~degree~"C"),
  "surf_temp" = expression("Surface Temperature"~degree~"C"),
  "bot_sal" = "Bottom Salinity",
  "surf_sal" = "Surface Salinity")
var_label <- var_titles[var_select]


# And Plot!
ggplot(data = filter(var_combined, ensemble_statistic == "Individual CMIP6 Output"), 
   aes_string(
     "date", var_select, group = "cmip_id", 
      color = "ensemble_statistic",
      size = "ensemble_statistic",
      alpha = "ensemble_statistic")) +
  geom_line() +
  geom_line(data = filter(var_combined, ensemble_statistic != "Individual CMIP6 Output")) +
  scale_color_manual(values = spaghetti_pal) +
  scale_size_manual(values = spaghetti_sizes) +
  scale_alpha_manual(values = spaghetti_alpha) +
  facet_wrap(~Region, ncol = 1, scales = "free") +
  labs(x = "", y = var_label, color = "Ensemble Statistic") +
  guides(
    color = guide_legend(title.position = "top", title.hjust = 0.5, nrow = 2),
    alpha = "none",
    size = "none")


# Plot just the 10th-90th-mean
var_combined %>% 
  filter(ensemble_statistic != "Individual CMIP6 Output") %>% 
  ggplot(
    aes_string(
     "date", var_select, group = "cmip_id", 
      color = "ensemble_statistic",
      size = "ensemble_statistic",
      alpha = "ensemble_statistic")) +
  geom_line() +
  scale_color_manual(values = spaghetti_pal) +
  scale_size_manual(values = spaghetti_sizes) +
  scale_alpha_manual(values = spaghetti_alpha) +
  facet_wrap(~Region, ncol = 1, scales = "free") +
  labs(x = "", y = var_label, color = "Ensemble Statistic") +
  scale_x_date(limits = as.Date(c("2000-01-01", "2050-12-31"))) +
  geom_vline(xintercept = as.Date("2015-01-01")) +
  guides(
    color = guide_legend(title.position = "top", title.hjust = 0.5, nrow = 2),
    alpha = "none",
    size = "none") +
  labs(title = "why is it pinching?")
```


```{r}
# ####  Export for later:  ####
# 
# # Folder To Put Table
# scenario_folder <- cs_path("res", str_c("CMIP6/", ssp_select, "/BiasCorrected/TimeseriesData/"))
# 
# # File Name
# ts_name <- str_c(scenario_folder, "CMIP6_bias_corrected_regional_", var_select, ".csv")
# print(str_c("Saving: ", ts_name))
# 
# # Save it
# write_csv(x = var_combined, file = ts_name)



```


## Prepare OISST/SODA/Glorys Timeseries


```{r}
# SODA vars
soda_ssal  <- stack(str_c(soda_path, "SODA_Salt_Red.nc")) # Has 50 depths, defaults to level 1 (surface)
soda_bsal  <- stack(str_c(soda_path, "SODA_Salt_Red_bottomLayer.nc"))
soda_btemp <- stack(str_c(soda_path, "SODA_Temp_Red_bottomLayer.nc"))


# Load Monthly data
oisst_month_path <- cs_path("res", "OISST/oisst_mainstays/monthly_averages")
oisst_monthly <- stack(str_c(oisst_month_path, "oisst_monthly.nc"), varname = "sst")

```



```{r}
# Perform the masking:

# Runs one variable at a time:
masked_surface_sal <- mask_and_stack(masking_var = soda_ssal, var_name = "surf_sal")
masked_bottom_sal  <- mask_and_stack(masking_var = soda_bsal, var_name = "bot_sal")
masked_bottom_temp <- mask_and_stack(masking_var = soda_btemp, var_name = "bot_temp")
masked_surf_temp   <- mask_and_stack(masking_var = oisst_monthly, var_name = "surf_temp")


# make months standardized for datetime joining
tune_months <- function(x){
  x  %>% 
    mutate(month =  str_pad(month(date), side = "left", pad = "0", width = 2), 
           date = as.Date(str_c(year,"-", month, "-15")))
}

masked_ssal <- tune_months(masked_surface_sal)
masked_stemp <- tune_months(masked_surf_temp)
masked_bsal <- tune_months(masked_bottom_sal)
masked_btemp <- tune_months(masked_bottom_temp)


```


### Check & Save


```{r}

####  Check & Save  ####

# Combine and add metadata
# "date" is going to need some tuning before join
references_combined <- left_join(
  masked_ssal, masked_stemp, join_by("Region", "date", "year", "month")) %>% 
  left_join(masked_bsal,join_by("Region", "date", "year", "month")) %>% 
  left_join(masked_btemp, join_by("Region", "date", "year", "month"))



# # Save these things out
# references_folder <- str_c(res_path, "CMIP6/Bias_Correction_Checking/bias_correction_reference_dataset_monthly_regional_means.csv")
# write_csv(
#   x = references_combined, 
#   file = references_folder
# )



```




